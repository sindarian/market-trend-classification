{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee24289",
   "metadata": {},
   "outputs": [],
   "source": [
    "import classifiers.labeller as labeller\n",
    "import classifiers.arima as arima\n",
    "import nn_train_driver\n",
    "import plotting.plot_shortcuts as ps\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.graphics.tsaplots import plot_pacf, plot_acf\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fc8c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in data\n",
    "data_df = pd.read_csv('./data/qqq_2022.csv')\n",
    "\n",
    "# produce truth labels\n",
    "print('Computing Label Space...')\n",
    "label_df = labeller.driver(data_df, 21, 'Close')\n",
    "print(label_df.head())\n",
    "\n",
    "# plot data/labels/label signal\n",
    "sig_col = 'Close'\n",
    "forecast_sig_col = 'Forecast ' + sig_col\n",
    "ps.plot_label_over_signal(data_df.iloc[-390:], label_df.iloc[-390:], sig_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc76a10",
   "metadata": {},
   "source": [
    "#### Labelling Explanation\n",
    "\n",
    "Stock Market data is very noisy, and is often likened to \"random walks\". Because of this, computing outright price differentials (price_t - price_t-1) will not result in coherent/noiseless data. So, some form of \"truth\" data must be computed.\n",
    "\n",
    "In order to best classify the direction of the market's movement, the following process was conducted to compute \"truth\" data:\n",
    "\n",
    "1. To account for growth, the coefficients of the following exponential function were optimized to the data:\n",
    "$\\newline$\n",
    "$y = Ax^B + C$\n",
    "$\\newline$\n",
    "\n",
    "2. To account for fluctuations around the exponential function, an FFT was computed on the following price values:\n",
    "$\\newline$\n",
    "$clean\\_signal = FFT(raw\\_data - growth\\_model)$\n",
    "$\\newline$\n",
    "\n",
    "3. Produce a clean version of the market data:\n",
    "$\\newline$\n",
    "$clean\\_market\\_signal = growth\\_model + clean\\_signal$\n",
    "$\\newline$\n",
    "\n",
    "4. Compute Velocity of clean market data\n",
    "$\\newline$\n",
    "$ velocity = \\frac{\\delta clean\\_market\\_signal}{\\delta t}$\n",
    "$\\newline$\n",
    "\n",
    "5. Compute Labels from Velocity\n",
    "$\\newline$\n",
    "$ y = 0;\\space velocity < 0 \\newline$\n",
    "$ y = 1;\\space velocity > 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd1da20",
   "metadata": {},
   "source": [
    "# ARIMA Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1f19e7-3a4e-4bdb-bb37-d1b4ac463925",
   "metadata": {},
   "source": [
    "## Choosing ARIMA Order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f103ff94-9c11-4b63-99e1-6077cb303ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The pacf and acf plots below are plotted twice due to an incompatability\n",
    "# between the stats package and jupyter notebooks. Adding a semicolon to the\n",
    "# end of the line solves the problem: https://github.com/statsmodels/statsmodels/issues/4155#issuecomment-445913264\n",
    "plot_pacf(data_df[[sig_col]].diff().dropna());\n",
    "\n",
    "# given the spike at 1 with the values significantly dropping off and\n",
    "# staying around 0, we use 1 as the initial autoregression param\n",
    "p = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162b66d8-ceab-4737-8395-20e5af00db06",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(3)\n",
    "ax1.plot(data_df[[sig_col]])\n",
    "ax1.set_title('Original Series')\n",
    "ax1.axes.xaxis.set_visible(False)\n",
    "\n",
    "ax2.plot(data_df[[sig_col]].diff())\n",
    "ax2.set_title('1st Order Differencing')\n",
    "ax2.axes.xaxis.set_visible(False)\n",
    "\n",
    "ax3.plot(data_df[[sig_col]].diff().diff())\n",
    "ax3.set_title('2nd Order Differencing')\n",
    "plt.show()\n",
    "\n",
    "# because the data appears relatively stabilized after a first order\n",
    "# difference, we use that as the d param\n",
    "d = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbe9da5-ecc1-4ed2-8d47-2f3a30021a8c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_acf(data_df[[sig_col]].diff().dropna());\n",
    "\n",
    "# given the spike at 1 with the values significantly dropping off and\n",
    "# staying around 0, we use 1 as the initial moving average param\n",
    "q = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f0709d-8874-49c9-81be-7ed3102f12b2",
   "metadata": {},
   "source": [
    "## Evaluating ARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ab4c33-fda2-45b6-b1e6-6fa2890a3867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# properly index the input data to ARIMA\n",
    "date_indexed_df = data_df.copy(deep=True)\n",
    "date_indexed_df['DTS'] = pd.to_datetime(date_indexed_df['EpochTime'], unit='s')\n",
    "date_indexed_df = date_indexed_df.set_index('DTS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dac9fa1-7c46-4314-b013-726c655a5d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up a day-wise dataframe to perform evaluation on\n",
    "true_values = date_indexed_df[date_indexed_df[sig_col].groupby(pd.Grouper(freq='D')).rank() == 1][1:][sig_col].to_frame()\n",
    "true_values.index = pd.to_datetime(true_values.index)\n",
    "true_values.index = true_values.index.strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c62da2-d1b3-4767-9041-92aba01c52ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fit and forecast with ARIMA\n",
    "forecast_df = arima.fit_forecast(date_indexed_df, sig_col, p, d, q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4dd0b15-a992-4429-922f-fa541039da80",
   "metadata": {},
   "outputs": [],
   "source": [
    "arima_df = pd.merge(true_values, forecast_df, left_index=True, right_index=True)\n",
    "arima_df = arima_df.rename(columns={1: forecast_sig_col})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41466da4-8acd-4efe-8623-898286557977",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the actual vs forecast values\n",
    "ps.plot_forecast(arima_df, sig_col, forecast_sig_col)\n",
    "\n",
    "# classify each data point as increasing or decreasing\n",
    "arima_classifications = arima.convert_forecast_to_classification(arima_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca7b988-8f39-4b54-bca1-b46aa96bfb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'\\nARIMA Classification Report:\\n{classification_report(arima_classifications[:,0], arima_classifications[:,1])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32787cca",
   "metadata": {},
   "source": [
    "# Neural Net Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b2744b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the default config file for building/training the Multilayer Perceptron\n",
    "nn_config = nn_train_driver.default_training_config()\n",
    "print(f'MLP Hyperparameter Configuration:')\n",
    "pprint(nn_config)\n",
    "\n",
    "# build/train the MLP\n",
    "print('\\nTraining MLP:')\n",
    "model, test_df = nn_train_driver.train_mlp(data_df, label_df=label_df, config=nn_config)\n",
    "\n",
    "# format the test df to just be the feature space\n",
    "feature_cols = list(test_df.columns)\n",
    "feature_cols.remove('EpochTime')\n",
    "feature_cols.remove('Label')\n",
    "\n",
    "# get the properly formatted test space\n",
    "y_test_true = test_df['Label']\n",
    "yhat_test = nn_train_driver.inverse_onehot(model.predict(test_df[feature_cols]))\n",
    "print(f'\\nTest Space Classification Report:\\n{classification_report(y_test_true, yhat_test)}')\n",
    "\n",
    "# plot the NN classifications on the last few days of data\n",
    "yhat_test_df = pd.DataFrame({'EpochTime': test_df['EpochTime'].values,\n",
    "                             'Label': yhat_test})\n",
    "ps.plot_label_over_signal(data_df.iloc[-390:], yhat_test_df.iloc[-390:], 'Close')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72940de4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "info629",
   "language": "python",
   "name": "info629"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
